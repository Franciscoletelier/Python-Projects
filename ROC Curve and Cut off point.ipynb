{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Determinación de la Curva ROC y el Cut off\n",
    "\n",
    "* Kernel Python 3.09.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "#-*- coding: utf-8 -*-\n",
    "\n",
    "# ! pip install --upgrade pip\n",
    "# !pip install sinfo\n",
    "# !pip install --upgrade numpy \n",
    "# !pip install plotly --upgrade\n",
    "# !pip install chart-studio --upgrade\n",
    "#!pip install ibm-cos-sdk #Documentación: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# !pip install boto3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instalamos las librerias necesarias\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "import plotly as plotly\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import numpy as np\n",
    "# import numpy.dual as dual --> Deprecado\n",
    "import scipy as sp\n",
    "from sklearn import metrics\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3 #Documentación: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "def __iter__(self): return 0\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Un-Comment these options if you want to exapand the number of rows and columns of you see visually in the notebook.\n",
    "#pd.set_option('display.max_columns', None)\n",
    "#pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Articulo\n",
    "\n",
    "* Link: https://medium.com/swlh/determining-a-cut-off-or-threshold-when-working-with-a-binary-dependent-target-variable-7c2342cf2a7c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Franciscoletelier/Python-Projects/df_for_export-2.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Franciscoletelier/Python-Projects/ROC Curve and Cut off point.ipynb Celda 5\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://github/Franciscoletelier/Python-Projects/ROC%20Curve%20and%20Cut%20off%20point.ipynb#W2sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Cargamos el dataset\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/Franciscoletelier/Python-Projects/ROC%20Curve%20and%20Cut%20off%20point.ipynb#W2sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m pd_data \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39mFranciscoletelier/Python-Projects/df_for_export-2.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, sep\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m,\u001b[39;49m\u001b[39m\"\u001b[39;49m, header\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1213\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1214\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[1;32m   1215\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[1;32m   1216\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[0;32m-> 1217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[1;32m   1218\u001b[0m     f,\n\u001b[1;32m   1219\u001b[0m     mode,\n\u001b[1;32m   1220\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1221\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1222\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[1;32m   1223\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[1;32m   1224\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1225\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   1226\u001b[0m )\n\u001b[1;32m   1227\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1228\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[0;32m/Applications/anaconda3/lib/python3.9/site-packages/pandas/io/common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[1;32m    785\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    786\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[1;32m    788\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[0;32m--> 789\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[1;32m    790\u001b[0m             handle,\n\u001b[1;32m    791\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[1;32m    792\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[1;32m    793\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[1;32m    794\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m    795\u001b[0m         )\n\u001b[1;32m    796\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    797\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[1;32m    798\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Franciscoletelier/Python-Projects/df_for_export-2.csv'"
     ]
    }
   ],
   "source": [
    "# Cargamos el dataset\n",
    "pd_data = pd.read_csv(\"Franciscoletelier/Python-Projects/df_for_export-2.csv\", sep=\",\", header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the minimum probability for each decile\n",
    "tips_summedv = pd.DataFrame(dfx.groupby(['DECILE'])['P_FAIL'].min())\n",
    "# Find the maximum probability of each decile\n",
    "tips_summedw = pd.DataFrame(dfx.groupby(['DECILE'])['P_FAIL'].max())\n",
    "# Find the Actual Failure rate for each decile.\n",
    "tips_summedx = pd.DataFrame(dfx.groupby(['DECILE'])['FAILURE_TARGET'].mean())\n",
    "#Sum the number of Failures in each decile.\n",
    "tips_summedy = pd.DataFrame(dfx.groupby(['DECILE'])['FAILURE_TARGET'].sum())\n",
    "# count the records in each decile\n",
    "tips_summedz = pd.DataFrame(dfx.groupby(['DECILE'])['FAILURE_TARGET'].count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aggregate the summaries into one dataframe\n",
    "tips = pd.concat([tips_summedv,tips_summedw, tips_summedx, tips_summedy,tips_summedz], axis=1)\n",
    "tips.columns = ['MIN_SCORE','MAX_SCORE','FAILURE_RATE','FAILURES', 'OBS']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips=tips.sort_values(by=['DECILE'], ascending=[False])\n",
    "gains=tips\n",
    "#Find the number of cumulative failures by decile.\n",
    "gains['CUML_FAILURES']=gains['FAILURES'].cumsum()\n",
    "#Find the percentage of failures in each decile\n",
    "gains['PCT_OF_FAILURES']=(gains.FAILURES)/(dfx['FAILURE_TARGET'].sum())*100\n",
    "#Find the cumulative percentage of failures in each decile.\n",
    "gains['CUML_PCT_OF_FAILURES']=gains.PCT_OF_FAILURES.cumsum()\n",
    "#Format the final output\n",
    "gains=gains[['OBS','MIN_SCORE','MAX_SCORE','FAILURES','FAILURE_RATE','PCT_OF_FAILURES','CUML_FAILURES','CUML_PCT_OF_FAILURES']]\n",
    "gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Finding the best cut-off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Franciscoletelier/Python-Projects/ROC Curve and Cut off point.ipynb Celda 10\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell://github/Franciscoletelier/Python-Projects/ROC%20Curve%20and%20Cut%20off%20point.ipynb#X12sdnNjb2RlLXZmcw%3D%3D?line=0'>1</a>\u001b[0m dfx\u001b[39m=\u001b[39mpd_data\n\u001b[1;32m      <a href='vscode-notebook-cell://github/Franciscoletelier/Python-Projects/ROC%20Curve%20and%20Cut%20off%20point.ipynb#X12sdnNjb2RlLXZmcw%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m#Add a small negative random number to P_FAIL to break ties when grouping\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://github/Franciscoletelier/Python-Projects/ROC%20Curve%20and%20Cut%20off%20point.ipynb#X12sdnNjb2RlLXZmcw%3D%3D?line=2'>3</a>\u001b[0m dfx[\u001b[39m'\u001b[39m\u001b[39mwookie\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m (np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39m0\u001b[39m, \u001b[39m100\u001b[39m, dfx\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]))\u001b[39m/\u001b[39m\u001b[39m100000000000000000\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd_data' is not defined"
     ]
    }
   ],
   "source": [
    "dfx=pd_data\n",
    "#Add a small negative random number to P_FAIL to break ties when grouping\n",
    "dfx['wookie'] = (np.random.randint(0, 100, dfx.shape[0]))/100000000000000000\n",
    "dfx['P_FAIL']=dfx['P_FAIL']+dfx['wookie']\n",
    "\n",
    "# Instead of deciles, we create 10000 groups, based on the probability to fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx['GROUPS'] = pd.qcut(dfx['P_FAIL'], 10000, labels=False)\n",
    "# find the minimum P_FAIL for each group.  This is a potential cut-off point.\n",
    "tips_summedb = pd.DataFrame(dfx.groupby(['GROUPS'])['P_FAIL'].min())\n",
    "#Find the number of Failures in each group\n",
    "tips_summedz = pd.DataFrame(dfx.groupby(['GROUPS'])['FAILURE_TARGET'].sum())\n",
    "#find the number of observations in each group\n",
    "tips_summeda = pd.DataFrame(dfx.groupby(['GROUPS'])['FAILURE_TARGET'].count())\n",
    "#append the summaries into one dataframe\n",
    "tips = pd.concat([tips_summedb,tips_summedz, tips_summeda], axis=1)\n",
    "tips.columns = ['CUT-OFF','FAILURES', 'OBS']\n",
    "#find the number of non-failures\n",
    "tips['NON_FAILURES']=tips.OBS-tips.FAILURES\n",
    "#reset the index to make GROUPS a column\n",
    "tips.reset_index(level=0, inplace=True)\n",
    "#sort the dataframe by groups in descending order\n",
    "tips=tips.sort_values(by=['GROUPS'], ascending=[False])\n",
    "# Cumulative sum the failures, non-failures and observations\n",
    "tips['INV_CUM_FAILURES'] = tips.FAILURES.cumsum()\n",
    "tips['INV_CUM_NON_FAILURES'] = tips.NON_FAILURES.cumsum()\n",
    "tips['TOTAL_OBS']=tips.OBS.sum()\n",
    "#Sort the data by Groups ascending\n",
    "tips=tips.sort_values(by=['GROUPS'], ascending=[True])\n",
    "#calculate the total number of failures and non-failures\n",
    "tips['CUM_FAILURES'] = tips.FAILURES.cumsum()\n",
    "tips['CUM_NON_FAILURES'] = tips.NON_FAILURES.cumsum()\n",
    "#find the total number of failures for the whole dataset.\n",
    "tips['TOTAL_FAILURES']=tips.FAILURES.sum()\n",
    "tips['TOTAL_NON_FAILURES']=tips.NON_FAILURES.sum()\n",
    "#define the true positives for each cut-off\n",
    "tips['TRUE_POSITIVES']=tips.INV_CUM_FAILURES\n",
    "#define the false positives for each cut-off\n",
    "tips['FALSE_POSITIVES']=tips.INV_CUM_NON_FAILURES\n",
    "#define the true negatives for each cut-off\n",
    "tips['TRUE_NEGATIVES']=tips.CUM_NON_FAILURES-tips.NON_FAILURES\n",
    "#define the false negatives for each cut-off\n",
    "tips['FALSE_NEGATIVES']=tips.CUM_FAILURES-tips.FAILURES\n",
    "#double check the logic and arithmetic.\n",
    "tips['OBS2']=tips.TRUE_POSITIVES+tips.FALSE_POSITIVES+tips.TRUE_NEGATIVES+tips.FALSE_NEGATIVES\n",
    "# define the sensitvity for each cut-off\n",
    "tips['SENSITIVITY']=tips['TRUE_POSITIVES']/(tips['TRUE_POSITIVES']+tips['FALSE_NEGATIVES'])\n",
    "#define the specificity for each cut-off\n",
    "tips['SPECIFICITY']=tips['TRUE_NEGATIVES']/(tips['FALSE_POSITIVES']+tips['TRUE_NEGATIVES'])\n",
    "#define the false positive rate for each cut-off\n",
    "tips['FALSE_POSITIVE_RATE']=1-tips['SPECIFICITY']\n",
    "#define the false negative rate for each cut-off\n",
    "tips['FALSE_NEGATIVE_RATE']=1-tips['SENSITIVITY']\n",
    "tipsx=tips\n",
    "gains=tipsx[['GROUPS','CUT-OFF','TRUE_POSITIVES','FALSE_POSITIVES','TRUE_NEGATIVES','FALSE_NEGATIVES','SENSITIVITY',\n",
    "            'SPECIFICITY','FALSE_POSITIVE_RATE','FALSE_NEGATIVE_RATE']]\n",
    "gains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Finding the cut-off with the smallest misclassification rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips=tipsx\n",
    "#sum the false positives and false negatives.\n",
    "tips['FALSE_CLASSIFICATIONS'] = tips.FALSE_POSITIVES+tips.FALSE_NEGATIVES\n",
    "#estimate the false classification rate\n",
    "tips['FALSE_CLASSIFICATION_RATE']=tips.FALSE_CLASSIFICATIONS/(tips.TOTAL_OBS)\n",
    "gains=tips[['GROUPS','CUT-OFF','TRUE_POSITIVES','FALSE_POSITIVES','TRUE_NEGATIVES','FALSE_NEGATIVES','SENSITIVITY',\n",
    "            'SPECIFICITY','FALSE_POSITIVE_RATE','FALSE_NEGATIVE_RATE','FALSE_CLASSIFICATIONS','FALSE_CLASSIFICATION_RATE']]\n",
    "gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tips['CUT-OFF']\n",
    "y1 = tips['FALSE_POSITIVE_RATE']\n",
    "y2 = tips['FALSE_NEGATIVE_RATE']\n",
    "y3 = tips['FALSE_CLASSIFICATION_RATE']\n",
    "trace = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y1,\n",
    "    name='False Positive Rate')\n",
    "trace2 = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y2,\n",
    "    name='False Negative Rate'\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y3,\n",
    "    name='False Classification Rate'\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title='Mis-Classification Rates BY CUT OFF SCORE',\n",
    "    xaxis=dict(\n",
    "        title='CUT OFF SCORE',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='False Positive and False Negative Rates',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True,\n",
    ")\n",
    "    \n",
    "data=[trace,trace2,trace3]  \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='styling-names')\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gains=gains.sort_values(by=['FALSE_CLASSIFICATION_RATE'], ascending=[True])\n",
    "gains=gains.head(1)\n",
    "gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx=pd_data\n",
    "dfx[‘Y_FAIL’] = np.where(((dfx.P_FAIL <= .909427)), 0, 1)\n",
    "print(pd.crosstab(dfx.Y_FAIL, dfx.FAILURE_TARGET, dropna=False))\n",
    "pd.crosstab(dfx.Y_FAIL, dfx.FAILURE_TARGET).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx=pd_data\n",
    "dfx['Y_FAIL'] = np.where(((dfx.P_FAIL <= .5)), 0, 1)\n",
    "print(pd.crosstab(dfx.Y_FAIL, dfx.FAILURE_TARGET, dropna=False))\n",
    "pd.crosstab(dfx.Y_FAIL, dfx.FAILURE_TARGET).apply(lambda r: r/r.sum(), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = tips['CUT-OFF']\n",
    "x1 = tips['FALSE_POSITIVE_RATE']\n",
    "y1 = tips['SENSITIVITY']\n",
    "trace = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y1,\n",
    "    name='ROC')\n",
    "trace2 = go.Scatter(\n",
    "    x = x2,\n",
    "    y = y1,\n",
    "    name='Cut-Off v TPR'\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title='ROC with Threshold Levels',\n",
    "    xaxis=dict(\n",
    "        title='False Positive Rate and Threshold Level',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='True Positive Rate',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True,\n",
    ")\n",
    "    \n",
    "data=[trace,trace2]  \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='styling-names')\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Finding the cut-off when the costs of false positives and false negatives are not equal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tips=tipsx\n",
    "#define the cost of a false positive and false negative\n",
    "cost_of_a_false_positive=1\n",
    "cost_of_a_false_negative=2\n",
    "#convert the costs into weights that sum to one\n",
    "fp_weight=(cost_of_a_false_positive)/(cost_of_a_false_positive+cost_of_a_false_negative)\n",
    "fn_weight=(cost_of_a_false_negative)/(cost_of_a_false_positive+cost_of_a_false_negative)\n",
    "#Create a weighted false classification rate based on the costs of a false positive and a false negative.\n",
    "tips['FALSE_CLASSIFICATIONS_W'] = np.where(((2*(((fp_weight)*tips.FALSE_POSITIVES+(fn_weight)*tips.FALSE_NEGATIVES)) >= tips.TOTAL_OBS)), tips.TOTAL_OBS, 2*(((fp_weight)*tips.FALSE_POSITIVES+(fn_weight)*tips.FALSE_NEGATIVES)))\n",
    "tips['FALSE_CLASSIFICATION_RATE_W']=tips.FALSE_CLASSIFICATIONS_W/(tips.TOTAL_OBS)\n",
    "gains=tips[['GROUPS','CUT-OFF','TRUE_POSITIVES','FALSE_POSITIVES','TRUE_NEGATIVES','FALSE_NEGATIVES','SENSITIVITY',\n",
    "            'SPECIFICITY','FALSE_POSITIVE_RATE','FALSE_NEGATIVE_RATE','FALSE_CLASSIFICATIONS_W','FALSE_CLASSIFICATION_RATE_W']]\n",
    "gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tips['CUT-OFF']\n",
    "y1 = tips['FALSE_POSITIVE_RATE']\n",
    "y2 = tips['FALSE_NEGATIVE_RATE']\n",
    "y3 = tips['FALSE_CLASSIFICATION_RATE_W']\n",
    "trace = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y1,\n",
    "    name='False Positive Rate')\n",
    "trace2 = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y2,\n",
    "    name='False Negative Rate'\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y3,\n",
    "    name='Weighted False Classification Rate'\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title='Weighted Mis-Classification Rates BY CUT OFF SCORE',\n",
    "    xaxis=dict(\n",
    "        title='CUT OFF SCORE',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Weighted False Positive and False Negative Rates',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True,\n",
    ")\n",
    "    \n",
    "data=[trace,trace2,trace3]  \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='styling-names')\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gains=gains.sort_values(by=['FALSE_CLASSIFICATION_RATE_W'], ascending=[True])\n",
    "gains=gains.head(1)\n",
    "gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx=pd_data\n",
    "dfx[‘Y_FAIL’] = np.where(((dfx.P_FAIL <= .8802)), 0, 1)\n",
    "print(pd.crosstab(dfx.Y_FAIL, dfx.FAILURE_TARGET, dropna=False))\n",
    "pd.crosstab(dfx.Y_FAIL, dfx.FAILURE_TARGET).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Finding a cut-off when the economic costs are known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the False Positive and False Negative Weights.\n",
    "cost_of_a_false_positive=2500\n",
    "cost_of_a_false_negative=27500\n",
    "fp_weight=(cost_of_a_false_positive)/(cost_of_a_false_positive+cost_of_a_false_negative)\n",
    "fn_weight=(cost_of_a_false_negative)/(cost_of_a_false_positive+cost_of_a_false_negative)\n",
    "#Define the False Classification Rate\n",
    "tips['FALSE_CLASSIFICATIONS_W'] = np.where(((2*(((fp_weight)*tips.FALSE_POSITIVES+(fn_weight)*tips.FALSE_NEGATIVES)) >= tips.TOTAL_OBS)), tips.TOTAL_OBS, 2*(((fp_weight)*tips.FALSE_POSITIVES+(fn_weight)*tips.FALSE_NEGATIVES)))\n",
    "tips['FALSE_CLASSIFICATION_RATE_W']=tips.FALSE_CLASSIFICATIONS_W/(tips.TOTAL_OBS)\n",
    "tips['TOTAL_COST']=tips.FALSE_POSITIVES*cost_of_a_false_positive+tips.FALSE_NEGATIVES*cost_of_a_false_negative\n",
    "gains=tips[['GROUPS','CUT-OFF','TRUE_POSITIVES','FALSE_POSITIVES','TRUE_NEGATIVES','FALSE_NEGATIVES','SENSITIVITY',\n",
    "            'SPECIFICITY','FALSE_POSITIVE_RATE','FALSE_NEGATIVE_RATE','FALSE_CLASSIFICATIONS_W','FALSE_CLASSIFICATION_RATE_W','TOTAL_COST']]\n",
    "gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tips['CUT-OFF']\n",
    "y1 = tips['FALSE_POSITIVE_RATE']\n",
    "y2 = tips['FALSE_NEGATIVE_RATE']\n",
    "y3 = tips['FALSE_CLASSIFICATION_RATE_W']\n",
    "trace = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y1,\n",
    "    name='False Positive Rate')\n",
    "trace2 = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y2,\n",
    "    name='False Negative Rate'\n",
    ")\n",
    "trace3 = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y3,\n",
    "    name='Weighted False Classification Rate'\n",
    ")\n",
    "layout = go.Layout(\n",
    "    title='Weighted Mis-Classification Rates BY CUT OFF SCORE',\n",
    "    xaxis=dict(\n",
    "        title='CUT OFF SCORE',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='False Positive and False Negative Rates',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True,\n",
    ")\n",
    "    \n",
    "data=[trace,trace2,trace3]  \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='styling-names')\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let’s examine the relationship between Total Costs and the cut-off or threshold level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tips['CUT-OFF']\n",
    "y1 = tips['TOTAL_COST']\n",
    "trace = go.Scatter(\n",
    "    x = x1,\n",
    "    y = y1,\n",
    "    name='Total Cost')\n",
    "layout = go.Layout(\n",
    "    title='Cut-Off Levels and Total Cost',\n",
    "    xaxis=dict(\n",
    "        title='CUT OFF SCORE',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    yaxis=dict(\n",
    "        title='Total Costs',\n",
    "        titlefont=dict(\n",
    "            family='Courier New, monospace',\n",
    "            size=18,\n",
    "            color='#7f7f7f'\n",
    "        )\n",
    "    ),\n",
    "    showlegend=True,\n",
    ")\n",
    "    \n",
    "data=[trace]  \n",
    "fig = go.Figure(data=data, layout=layout)\n",
    "#plot_url = py.plot(fig, filename='styling-names')\n",
    "plotly.offline.iplot(fig, filename='shapes-lines')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gains=gains.sort_values(by=['TOTAL_COST'], ascending=[True])\n",
    "gains=gains.head(1)\n",
    "gains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx['Y_FAIL'] = np.where(((dfx.P_FAIL <= .665541)), 0, 1)\n",
    "print(pd.crosstab(dfx.Y_FAIL, dfx.FAILURE_TARGET, dropna=False))\n",
    "pd.crosstab(dfx.Y_FAIL, dfx.FAILURE_TARGET).apply(lambda r: r/r.sum(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.0 Summary and conclusions\n",
    "* As you can see from these three scenarios, the best cut-off will depend greatly on the economics of your problem. Because of this, it is important to understand economic costs of both a false positive and a false negative. Just like most things in data science. The “best” answer depends on the context. If a false negative means a bolt will fly off and poke someone’s eye out, you have to make sure your model predictions reflect this hazard. Data science, like all things in this world, is subject to the context in which it is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
